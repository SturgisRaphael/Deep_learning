{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLSbN8DymegE"
   },
   "source": [
    "# TP 4 - Deep Learning\n",
    "# Master IAAA - Master DS - 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRkeE_P-DGPS"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF_2BMMdd8JK"
   },
   "outputs": [],
   "source": [
    "### Pour utiliser sur google colab\n",
    "\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')\n",
    "\n",
    "!mkdir -p drive -v\n",
    "#!google-drive-ocamlfuse drive\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#### Changez le chemin ci-dessous vers votre repertoire dans votre googledrive\n",
    "##############################################################################\n",
    "\n",
    "monchemin = 'drive/My Drive/DeepLearningCourse/Hyperas/'\n",
    "\n",
    "dir_path  = os.path.join(cwd, monchemin)\n",
    "dirs = os.listdir(dir_path)\n",
    "os.chdir(dir_path)\n",
    "\n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7JJMzYamegG"
   },
   "source": [
    "## 1. Comparaisons des algorithmes de gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnWABASVmegH"
   },
   "source": [
    "###  Convnet pour la classification de Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mEvn7plmegI"
   },
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2XScc1OmegK"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f3e1f9922d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "import keras\n",
    "from keras.engine import Layer\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, BatchNormalization, concatenate, Reshape, Conv2DTranspose, Activation\n",
    "\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu2Qj07amegN"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_datas():\n",
    "\t# input image dimensions\n",
    "\timg_rows, img_cols = 28, 28\n",
    "\tnb_classes = 10\n",
    "\n",
    "\t##### Chargement des donnees\n",
    "\n",
    "\t# the data, shuffled and split between train and test sets\n",
    "\t(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "\tprint (K.image_dim_ordering())\n",
    "\n",
    "\tif K.image_dim_ordering() == 'th':\n",
    "\t\tX_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "\t\tX_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "\t\tinput_shape = (1, img_rows, img_cols)\n",
    "\telse:\n",
    "\t\tX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "\t\tX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\t\tinput_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "\tX_train = X_train.astype('float32')\n",
    "\tX_test = X_test.astype('float32')\n",
    "\tX_train = X_train /255\n",
    "\tX_test = X_test /255\n",
    "\tprint('X_train shape:', X_train.shape)\n",
    "\tprint('X_test shape:', X_test.shape)\n",
    "\tprint(X_train.shape[0], 'train samples')\n",
    "\tprint(X_test.shape[0], 'test samples')\n",
    "\n",
    "\t# convert class vectors to binary class matrices\n",
    "\ty_train = keras.utils.to_categorical(Y_train, nb_classes)\n",
    "\ty_test = keras.utils.to_categorical(Y_test, nb_classes)\n",
    "\tprint('y_train shape:', y_train.shape)\n",
    "\tprint('y_test shape:', y_test.shape)\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVO5iGlVmegR"
   },
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "nb_classes =10 \n",
    "X_train, Y_train, X_test, Y_test = init_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7RE7yCtwmegY"
   },
   "outputs": [],
   "source": [
    "### Added 4 10 2018\n",
    "def create_discri(opt= 'Adam'):\n",
    "    d_input = Input(shape=(28, 28, 1) ,name=\"input\")\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu') (d_input)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    d_prediction = Dense(nb_classes, activation='softmax')(x)\n",
    "    discriminatorC = Model(input = d_input, output = d_prediction)\n",
    "    discriminatorC.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[metrics.categorical_accuracy])\n",
    "    return discriminatorC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do \n",
    "\n",
    "Comparez les routines d'optimisation Adam, Adagrad, Adadelta etc en utiisant dfférents learning rates poru l'apprentissage des modèles défnis ci-dessus\n",
    "\n",
    "Vous tracerez deux figures, l'une pour l'accuracy, l'autre pour le loss, calculés sur le test set.\n",
    "\n",
    "Dans chaque figure vous ploitterez la quantité (Accuracy ou Loss) en fonction du learning rate en incluant une courbe par routine d'optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnEeTSJRmego"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bLQpQpgmegi"
   },
   "outputs": [],
   "source": [
    "### Added 4 10 2018\n",
    "def create_discri(opt= 'Adam', DO_rate1=0.5, DO_rate2=0.5):\n",
    "    d_input = Input(shape=(28, 28, 1) ,name=\"input\")\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu') (d_input)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(DO_rate1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(DO_rate2)(x)\n",
    "    d_prediction = Dense(nb_classes, activation='softmax')(x)\n",
    "    discriminatorC = Model(input = d_input, output = d_prediction)\n",
    "    discriminatorC.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[metrics.categorical_accuracy])\n",
    "    return discriminatorC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4q31Ampjmegk"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Adagrad, Adadelta\n",
    "\n",
    "Perfs={}\n",
    "T_DO1 = [0., 0.25, 0.5]\n",
    "T_DO2 = [0., 0.25, 0.5]\n",
    "\n",
    "Optimizers = [\"Adam\", \"Adagrad\", \"Adadelta\"]\n",
    "for opt in Optimizers:\n",
    "    Perfs[opt]={}\n",
    "    for do1 in T_DO1:\n",
    "      for do2 in T_DO2:\n",
    "        o = eval(opt)()\n",
    "        D = create_discri(opt=o, DO_rate1 = do1 , DO_rate2 = do2)\n",
    "        D.fit(X_train, Y_train, epochs=1)\n",
    "        Perfs[opt][str(do1), str(do2)]=D.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2339KMAumege"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (Perfs[\"Adam\"])\n",
    "\n",
    "print (Perfs[\"Adagrad\"])\n",
    "\n",
    "print (Perfs[\"Adadelta\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8FVaeMDmego"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pxt9PmWtmegs"
   },
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BAXMarrmegv"
   },
   "source": [
    "### To do \n",
    "\n",
    "1. Rajoutez une couche de BatchNormalization et expliquez le nombre de paramètres trainable et not trainable\n",
    "\n",
    "\n",
    "2. Calculez les statistiques sur les activations moyennes (et les variances)  dans les différentes couches. Vous devez apprendre un réseau avec BN et un autre sans, puis faire passer les données d'apprentissage dans le réseau et extraire les prédiction sur les différentes couches cachées, puis afficher les valeurs moyennes d'activation sur les différentes couches \n",
    "\n",
    "NB: Le comportement de la couche de BN est plus visible avec des réseaux profonds non convolutionnels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSfz3K0Fmegw"
   },
   "source": [
    "## Optimisation des hypermaramètres\n",
    "\n",
    "Quel que soit le problème, quelles que soient vos données, il existe un certain nombre d'hyper paramètres à régler pour apprndre un bon modèle (architceture du modèle, paramètres de l'algorithme d'optimisation etc).\n",
    "\n",
    "C'est l'objet du gridsearch, qui peut être exhaustif ou pas.\n",
    "\n",
    "Avant de vous lancer dans un gridsearch faites une estumation du temps que cela prendra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9jqf5d5megx"
   },
   "source": [
    "### Gridsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1vcR7DVysay"
   },
   "source": [
    "En utilisant un embedding des modèles dans sklearn et le gridsearchCV de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkqsTPoHmegy"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M01i8Hyqmeg1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_discri_BN(optimizer='rmsprop', nb_filters1= 64,  nb_filters2= 32, fact = 'relu', nb_hid = 128, do_rate= 0.5, BN=False):\n",
    "    d_input = Input(shape=(28, 28, 1) ,name=\"input\")\n",
    "    x = Conv2D(nb_filters1, kernel_size=(3, 3), activation='relu') (d_input)\n",
    "    x = Conv2D(nb_filters2, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(nb_hid)(x)\n",
    "    x = Activation(fact)(x)\n",
    "    x = Dropout(do_rate)(x)\n",
    "    d_prediction = Dense(nb_classes, activation='softmax')(x)\n",
    "    discriminatorC = Model(input = d_input, output = d_prediction)\n",
    "    discriminatorC.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return discriminatorC\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCQRgYenyy6k"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = KerasClassifier(build_fn=create_discri_BN)\n",
    "\n",
    "optimizers = ['rmsprop']\n",
    "epochs = [10]\n",
    "V_nb_hid = [64, 128]\n",
    "DO_rate=[0, 0.25, 0.5]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers,  nb_hid =V_nb_hid, do_rate= DO_rate)\n",
    "\n",
    "print (param_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, Y_train, epochs = 1, verbose=2)\n",
    "# summarize results                                                                                                                                           \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsmqaTjlyzia"
   },
   "source": [
    "Si vous utilisez un cluster de calcul, il est souvent plus simple \n",
    "- De lancer un job par jeu d'hyperparametres\n",
    "=> Boucles énumérant toutes les conbinaisons : Un job lancé à chaque fois\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QWVeoXbyz06"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbnIEkN4meg3"
   },
   "source": [
    "### Optimisation Hyperas \n",
    "Cf. https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf\n",
    "Stratégie d'exploration partielle et informée de l'espace de recherche \n",
    "\n",
    "La méthode explore une partie de l'espace de recherche en construisant au fur et à mesure une mesure d'intérêt des différets paramètres à faire varier (voir l'article cité cidessus).\n",
    "\n",
    "Nb: Pour utiliser hyperas avec google colab votre notebook doit etre sauvegardé sur google colab et vous devez spécifier son chemin (voir cellule suivante).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65xwvdLpmeg5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install hyperas\n",
    "!pip install hyperopt\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "\n",
    "def init_datas():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    nb_classes = 10\n",
    "\n",
    "    ##### Chargement des donnees\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    #X_train = X_train.reshape((X_train.shape[0],784))\n",
    "    X_train = X_train.reshape(X_train.shape[0], 784) \n",
    "    X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "  \n",
    "  \n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train = X_train /255\n",
    "    X_test = X_test /255\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = keras.utils.to_categorical(Y_train, nb_classes)\n",
    "    Y_test = keras.utils.to_categorical(Y_test, nb_classes)\n",
    "    print('y_train shape:', Y_train.shape)\n",
    "    print('y_test shape:', Y_test.shape)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def create_model(X_train, Y_train, X_test, Y_test):\n",
    "   \n",
    "    \"\"\"Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\"\"\"\n",
    " \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([64, 128, 256])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              epochs=1,\n",
    "              verbose=2,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "init_datas()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qlIW15hmeg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=init_datas,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(), notebook_name='TP4_DL_2018_OptimDNN')\n",
    "X_train, Y_train, X_test, Y_test = init_datas()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vhu1PzIq869F"
   },
   "source": [
    "## To do : Régularisation et taille de l'ensemble d'apprentissage\n",
    "\n",
    "- Trouver la meilleure stratégie d'apprentissage pour une architcteure neuronale donnée, sur les données Mnist et pour des jeux de données limités. Vous utiliserez plusieurs tirages random pour chaque taille et calculerez des résultats moyennés. Vous utiliserez deux tailles de base d'apprentissage.\n",
    "--  1000 données d'apprentissage en tout (environ 100 par classe)\n",
    "-- 5000 données d'apprentissage en tout (environ 500) ? \n",
    "\n",
    "- Vous discuterez suit à vos résultats de l'intérêt de la régularisation dans le cas de données limitées et de l'Intérêt des différentes techniques de regularisation(dropout, pénalisation L1, L2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_DL_2018_OptimDNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
